{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Code\n",
    "This is a code along of the famous titanic dataset, its always nice to start off with this dataset because it is an example you will find across pretty much every data analysis language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true').options(inferSchema='true').load(\"file:/titanic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_cols = data.select(['Survived',\n",
    " 'Pclass',\n",
    " 'Name',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Ticket',\n",
    " 'Fare',\n",
    " 'Cabin',\n",
    " 'Embarked'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: double (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_final_data_drop = my_cols.na.drop()\n",
    "from pyspark.sql.types import DoubleType\n",
    "my_final_data = my_final_data_drop.withColumn(\"Survived\", my_final_data_drop.Survived.cast(DoubleType()))\n",
    "my_final_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Categorical Columns\n",
    "\n",
    "Let's break this down into multiple steps to make it all clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='Sex',outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex',outputCol='SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer(inputCol='Embarked',outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkIndex',outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    " 'SexVec',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'EmbarkVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines \n",
    "\n",
    "Let's see an example of how to use pipelines (we'll get a lot more practice with these later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer,embark_indexer,\n",
    "                           gender_encoder,embark_encoder,\n",
    "                           assembler,log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+----+-----+-----+-----------+--------+-----------+--------+\n",
      "|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|     Ticket|    Fare|      Cabin|Embarked|\n",
      "+--------+------+--------------------+------+----+-----+-----+-----------+--------+-----------+--------+\n",
      "|     0.0|     1|Baxter, Mr. Quigg...|  male|24.0|    0|    1|   PC 17558|247.5208|    B58 B60|       C|\n",
      "|     0.0|     1|Blackwell, Mr. St...|  male|45.0|    0|    0|     113784|    35.5|          T|       S|\n",
      "|     0.0|     1|Carrau, Mr. Franc...|  male|28.0|    0|    0|     113059|    47.1|           |       S|\n",
      "|     0.0|     1|Chaffee, Mr. Herb...|  male|46.0|    1|    0|W.E.P. 5734|  61.175|        E31|       S|\n",
      "|     0.0|     1|   Fortune, Mr. Mark|  male|64.0|    1|    4|      19950|   263.0|C23 C25 C27|       S|\n",
      "|     0.0|     1|Futrelle, Mr. Jac...|  male|37.0|    1|    0|     113803|    53.1|       C123|       S|\n",
      "|     0.0|     1|  Giglio, Mr. Victor|  male|24.0|    0|    0|   PC 17593|    79.2|        B86|       C|\n",
      "|     0.0|     1|Goldschmidt, Mr. ...|  male|71.0|    0|    0|   PC 17754| 34.6542|         A5|       C|\n",
      "|     0.0|     1|Graham, Mr. Georg...|  male|38.0|    0|    1|   PC 17582|153.4625|        C91|       S|\n",
      "|     0.0|     1|Harris, Mr. Henry...|  male|45.0|    1|    0|      36973|  83.475|        C83|       S|\n",
      "|     0.0|     1|Holverson, Mr. Al...|  male|42.0|    1|    0|     113789|    52.0|           |       S|\n",
      "|     0.0|     1|Isham, Miss. Ann ...|female|50.0|    0|    0|   PC 17595| 28.7125|        C49|       C|\n",
      "|     0.0|     1|Meyer, Mr. Edgar ...|  male|28.0|    1|    0|   PC 17604| 82.1708|           |       C|\n",
      "|     0.0|     1|Natsch, Mr. Charl...|  male|37.0|    0|    1|   PC 17596|    29.7|       C118|       C|\n",
      "|     0.0|     1| Partner, Mr. Austen|  male|45.5|    0|    0|     113043|    28.5|       C124|       S|\n",
      "|     0.0|     1| Ringhini, Mr. Sante|  male|22.0|    0|    0|   PC 17760|135.6333|           |       C|\n",
      "|     0.0|     1|Silvey, Mr. Willi...|  male|50.0|    1|    0|      13507|    55.9|        E44|       S|\n",
      "|     0.0|     1|Smith, Mr. James ...|  male|56.0|    0|    0|      17764| 30.6958|         A7|       C|\n",
      "|     0.0|     1|Stead, Mr. Willia...|  male|62.0|    0|    0|     113514|   26.55|        C87|       S|\n",
      "|     0.0|     1|Uruchurtu, Don. M...|  male|40.0|    0|    0|   PC 17601| 27.7208|           |       C|\n",
      "+--------+------+--------------------+------+----+-----+-----+-----------+--------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_titanic_data, test_titanic_data = my_final_data.randomSplit([0.7,.3])\n",
    "train_titanic_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "results = fit_model.transform(test_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|     0.0|       1.0|\n",
      "|     0.0|       1.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('Survived','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|     0.0|       1.0|\n",
      "|     0.0|       1.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('Survived','prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
